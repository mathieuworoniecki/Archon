# Meilleurs RAG open source à étudier pour améliorer Archon

## Contexte et critères de sélection pour un RAG d’investigation

Votre projet **Archon** ressemble davantage à une **plateforme d’investigation documentaire** qu’à un simple chatbot : ingestion de gros volumes hétérogènes (PDF/images/vidéos/emails), indexation asynchrone, recherche hybride (lexicale + sémantique) et vues analytiques (timeline, graphe d’entités, galerie, viewer, chat). Dans ce contexte, “les meilleurs RAG open source” ne sont pas uniquement ceux qui “répondent bien”, mais ceux qui apportent des patterns réutilisables pour : **qualité de récupération**, **traçabilité/citations**, **gestion de documents complexes**, **évaluation/observabilité**, et **industrialisation**. citeturn4search2turn3search15

Le point fort d’Archon est déjà très proche d’un “RAG sérieux” : vous séparez l’indexation (workers) et le serving (API), et vous combinez deux moteurs complémentaires (lexical + vectoriel). La littérature et les implémentations open source convergent sur l’idée que l’hybride “keyword + sémantique” est souvent supérieur à l’un ou l’autre seul, car il combine précision lexicale et rappel sémantique. citeturn7search7turn7search14

Dans ce rapport, j’ai priorisé des projets **open source** (ou “source available” clairement signalés) repérés sur entity["company","GitHub","code hosting platform"] et leurs documentations officielles, en évaluant : (a) licence et implications, (b) maturité/usage production, (c) adéquation technique avec une stack Python/FastAPI et une recherche hybride (vous utilisez entity["company","Meilisearch","search engine"] et entity["company","Qdrant","vector database"]), (d) présence de briques clés (ingestion, chunking, reranking, citations, éval). citeturn7search2turn7search0turn7search14

## Frameworks RAG “composables” à étudier comme bibliothèques

Ces frameworks servent surtout de **réservoirs d’implémentations** et de patterns (retrievers, splitters, routers, pipelines, agents, outils d’évaluation). Pour Archon, l’intérêt est rarement de “remplacer” votre pipeline Celery, mais plutôt de **copier/adapter** des composants et stratégies.

**entity["company","LangChain","llm framework company"] (MIT)** est un framework pour assembler composants et intégrations (LLMs, vector stores, loaders, etc.) afin de construire des applications et agents LLM. Le dépôt et la FAQ officielle indiquent explicitement une licence MIT et mettent en avant un modèle “composants interopérables”, ainsi que LangGraph pour des workflows d’agents contrôlables. citeturn8search8turn8search0turn8search4 Un signal très concret de compatibilité : la doc d’intégration montre comment transformer un vector store en retriever (dont les variantes type MMR), et inclut un guide Qdrant. citeturn7search2turn7search30 Pour Archon, LangChain est utile comme “catalogue” d’options : query rewriting/multi-query, retrievers spécialisés, rerankers, patterns d’assemblage du contexte, et instrumentation via tooling externe.

**entity["company","LlamaIndex","llm data framework company"] (MIT)** est orienté “data-first” : ingestion/structuration (indices, graphes) et interface de requêtage au-dessus de vos données. Le dépôt met en avant connecteurs, structuration en indices/graphes et interfaces de query, avec licence MIT. citeturn0search1turn8search1turn8search5 L’intérêt pour Archon est la **boîte à outils de stratégies RAG** (index types, routing/fusion, “query engines”) plutôt que l’UI. De plus, on trouve de nombreux exemples de RAG de base (et au-delà) construits sur LlamaIndex, ce qui facilite le “copier-coller conceptuel” de patterns validés. citeturn0search7turn8search37

**Haystack par entity["company","deepset","ai company"] (Apache-2.0)** se positionne explicitement comme framework open source “production-ready” pour RAG et agents, avec une approche pipeline modulaire et inspectable. citeturn0search6turn0search0turn0search3 Le plus intéressant pour vous est qu’ils publient aussi un exemple d’application RAG : backend en FastAPI + Haystack 2 (et UI React), ce qui ressemble structurellement à votre découpage “API + UI”. citeturn0search10 Même si vous gardez vos services (Meilisearch/Qdrant), l’approche “pipeline explicite” de Haystack est très réutilisable pour tracer les étapes RAG, injecter un reranker, ou rendre l’explication/citation plus robuste.

**DSPy (MIT), issu du travail de entity["organization","Stanford University","university california us"]**, est différent : l’objectif est de “programmer plutôt que prompting” et d’optimiser algorithmiquement prompts/poids pour des pipelines (dont RAG). citeturn1search0turn8search2turn1search20 Pour Archon, DSPy est surtout une piste si vous voulez une **amélioration systématique** de la qualité (ex. compilations/optimisations de prompts de citation, reformulation de requêtes, sélection de passages) plutôt que des heuristiques “à la main”.

**GraphRAG de entity["company","Microsoft","technology company"] (MIT)** vise à extraire une structure (entités/relations + résumés) depuis un corpus pour augmenter les prompts à la requête. citeturn1search1turn8search3turn1search9 Le point clé est l’idée “graph + community summaries” pour mieux répondre sur des corpus narratifs/privés sans se limiter à une recherche vectorielle sur chunks. citeturn1search9turn1search21 Étant donné qu’Archon a déjà une vue “graphe d’entités”, GraphRAG est particulièrement pertinent à étudier pour “industrialiser” la partie extraction/agrégation et produire des contextes plus structurés que des chunks bruts. citeturn8search7turn8search10

image_group{"layout":"carousel","aspect_ratio":"16:9","query":["retrieval augmented generation architecture diagram hybrid search reranker","Haystack RAG pipeline diagram","Microsoft GraphRAG knowledge graph diagram"]}

## Moteurs et plateformes RAG open source prêts à déployer

Ces projets sont plus “verticalisés” : ingestion + index + API + parfois UI. Même si vous n’allez pas les adopter tels quels, ils sont excellents à “donner à votre IA dans l’IDE” car ils exposent des décisions implémentées (chunking, citations, ACL, UIs d’investigation).

**RAGFlow (Apache-2.0)** se présente comme un moteur RAG open source combinant workflow RAG et capacités “agent”, avec un accent sur la compréhension profonde de documents et des citations “bien fondées”. Le dépôt est très populaire et le guide “Get started” décrit explicitement un serveur local, la création de dataset et une chaîne parsing → chat sur datasets avec citations. citeturn1search6turn1search22turn1search2 Pour Archon (PDF complexes, OCR, pièces jointes), RAGFlow vaut surtout pour son approche “ETL for AI data” et sa logique de parsing/structuration avant embedding. citeturn1search14turn1search22 À noter : un SDK Python a eu une release récente (11 février 2026), signe d’un écosystème actif et consommable en API. citeturn1search26

**Onyx (ex-Danswer) – dépôt historique sous entity["company","UiPath","automation software company"], licence CE MIT** : le repo décrit une plateforme connectée aux docs/apps/personnes, avec UI, agents, et de nombreux connecteurs (Drive, Slack, Confluence, Salesforce…) en conservant des contrôles d’accès. citeturn1search15turn9search4 Le dépôt “Onyx” annonce deux éditions : Community Edition sous MIT et Enterprise Edition avec fonctionnalités additionnelles. citeturn9search4turn9search0 Un autre angle utile (même si Archon est “dossiers locaux”) est l’industrialisation : permissioning, rôles, sécurité, et un modèle mental “enterprise search + RAG”. citeturn1search3turn9search8

**R2R (MIT) par entity["company","SciPhi","ai retrieval company"]** se positionne comme un système de retrieval/RAG “production-ready” autour d’une API REST, avec ingestion multimodale, recherche hybride, knowledge graphs, gestion documentaire, et même un “Deep Research API” (multi-étapes, data interne et/ou web). citeturn4search3turn10search3turn4search25 Pour Archon, R2R est une excellente source d’inspiration sur : (a) design d’API RAG en production, (b) orchestration d’une recherche “agentic”, (c) structuration des abstractions retrieval → context → answer. citeturn4search3turn10search27

**Dify par entity["company","LangGenius","llmops company"]** est une plateforme open source pour développer des applications LLM combinant workflows “agentic”, pipelines RAG, gestion de modèles et observabilité. citeturn4search0turn4search4 Le point d’attention est la licence : le dépôt indique une “Dify Open Source License” basée sur Apache 2.0 mais avec des conditions additionnelles. citeturn9search5turn9search1 Si votre projet Archon vise une compatibilité stricte OSI/permissive, ce détail est important à auditer.

**Langflow (MIT)** est une plateforme visuelle pour construire et déployer des agents/workflows, avec serveurs API et MCP intégrés (chaque workflow peut devenir un “tool” intégrable). citeturn4search1turn4search27turn9search2 L’intérêt pour Archon est surtout l’outillage de prototypage : tester rapidement variantes de retrieval/reranking/prompting avant de les recoder “proprement” dans votre backend.

**Open WebUI** est une interface self-hosted orientée “standards”, avec une fonctionnalité RAG capable d’ingérer documents locaux/distants et de supporter plusieurs bases vectorielles et moteurs d’extraction (dont Tika/Docling), selon le README et la doc RAG. citeturn4search5turn4search2turn4search15 Deux alertes à intégrer dans votre diligence :
- **Licence** : à partir de la v0.6.6 (19 avril 2025), le projet indique une clause de protection de marque/branding, tout en précisant que les contributions jusqu’à v0.6.5 restent sous BSD-3. citeturn9search3  
- **Sécurité** : une advisory GitHub décrit une vulnérabilité de code injection via “Direct Connections” (SSE `execute` events) menant à vol de tokens et pouvant être chaînée vers du RCE via l’API “Functions” sur certaines configurations. citeturn11search1turn11search2turn11search0  
Même si Archon n’utilise pas Open WebUI, ces éléments sont instructifs “par analogie” : dès que vous autorisez des connexions à des endpoints externes/modèles, vous devez traiter ces endpoints comme non fiables.

**Quivr (Apache-2.0)** se présente comme un RAG “opinionated” pour intégrer GenAI dans des apps, multi-LLM et multi-vectorstores, avec possibilité de custom parsers. citeturn10search4turn10search0turn2search25 Pour votre pipeline, l’intérêt secondaire mais très concret est **MegaParse**, un parseur orienté ingestion LLM (PDF/DOCX/PPTX) open source sous Apache-2.0. citeturn10search36turn2search32

**txtai (Apache-2.0)** est un framework “all-in-one” autour d’une base d’embeddings combinant index (sparse+dense), graphes et base relationnelle, avec des workflows incluant RAG et multimodal (audio/images/vidéo) selon le README. citeturn2search0turn2search12 Sa doc RAG formalise l’idée “prompt + datastore de contexte + modèle génératif”, ce qui en fait un bon codebase à étudier pour la génération de citations et la gestion d’un datastore unifié. citeturn2search7 Le site de documentation indique aussi que txtai est bâti avec Transformers/Sentence Transformers et FastAPI, et rappelle la licence Apache-2.0 (utile pour une architecture comme Archon). citeturn10search5

**llmware (Apache-2.0)** est un framework orienté “enterprise RAG” et insiste sur des aspects comme citation de sources, fact checking et guardrails dans ses descriptions/ressources. citeturn2search1turn10search2turn10search18 Pour Archon, llmware est surtout intéressant comme base d’inspiration sur la **gouvernance de réponse** (citations, contrôles anti-hallucinations) dans des environnements “sensibles”. citeturn10search18turn2search5

## Briques open source à réutiliser directement dans votre pipeline

La qualité du RAG dans un environnement “investigation documentaire” dépend fortement de (a) la **qualité d’extraction/structure**, (b) la **stratégie de chunking**, (c) l’**embedding**, (d) le **reranking**, (e) la **fusion hybride**. Les projets ci-dessous sont souvent les “vrais multiplicateurs” de qualité, plus que le framework d’orchestration.

### Extraction et structuration de documents complexes

**Apache Tika** (projet de la entity["organization","Apache Software Foundation","open source foundation"]) est un standard de facto pour détecter le type de fichier et extraire texte + métadonnées via une interface unique, avec une couverture annoncée de “plus d’un millier” de formats (PPT/XLS/PDF, etc.). citeturn7search0turn7search1 Pour Archon, Tika est une brique “investigation-friendly” (emails/archives/office) qui peut renforcer votre extraction avant indexation, notamment si vous voulez uniformiser le traitement multi-formats.

**Unstructured** (société/écosystème entity["company","Unstructured","document processing company"]) publie une bibliothèque open source d’ingestion/préprocessing de documents (PDF/HTML/Word/images…) orientée LLM. citeturn6search0turn6search8turn12search3 Leur documentation insiste sur une sortie “éléments” (titres, texte narratif, listes, tables, images…) avec métadonnées, ce qui aide directement à (1) chunker intelligemment, (2) générer des citations plus précises (page/section), (3) alimenter un graphe d’entités avec des spans mieux localisés. citeturn6search33turn6search14turn6search21

**Docling (open source, lié à entity["company","IBM","technology company"])** vise la conversion de documents en représentations structurées (Markdown/JSON/HTML) et met en avant une compréhension PDF plus “riche” que de l’extraction texte brute. citeturn6search2turn6search6turn6search13 Le bénéfice typique côté RAG est l’accès à l’ordre de lecture, la détection de tableaux/formules, et une structure réutilisable pour le chunking et la citation. citeturn6search27turn6search23

**Marker** convertit des documents (PDF/images/Office/HTML/EPUB…) vers Markdown/JSON/chunks, en gérant tableaux, code, images et suppression d’artefacts (headers/footers), avec option d’amélioration par LLM. citeturn6search3turn6search28 Pour Archon, Marker est particulièrement intéressant si votre UX “viewer + citations” bénéficierait d’un format Markdown fidèle et de chunks “proches du document” (ex. sections + tables), plutôt qu’un texte linéaire OCR bruité. citeturn6search3

### Embeddings multilingues et reranking

Côté embeddings, deux familles très utilisées et faciles à self-host via entity["company","Hugging Face","ai platform company"] (ou équivalents) sont :

- **BGE-M3** (écosystème BGE développé par la entity["organization","Beijing Academy of Artificial Intelligence","research institute beijing"]) : le modèle est présenté comme multi-fonction (dense, sparse, multi-vector), multi-lingue (100+ langues) et capable de travailler sur des entrées longues (jusqu’à 8192 tokens), ce qui est pertinent pour un corpus FR + pièces longues d’investigation. citeturn5search0turn12search14  
- **multilingual-e5-large** : modèle d’embedding multilingue documenté avec détails techniques (couches, dimension) et usage retrieval, souvent utilisé pour la similarité sémantique et la récupération. citeturn5search1turn5search5

Pour augmenter fortement la précision top-k (et donc la qualité des réponses/citations), un **reranker cross-encoder** après votre retrieval hybride est une pratique très fréquente : les modèles “BGE reranker” décrivent explicitement le schéma “récupérer top N via embeddings, puis reranker pour obtenir top 3”. citeturn5search2turn5search32 C’est l’un des retours sur investissement les plus élevés dans les stacks RAG open source, car cela corrige beaucoup de “faux positifs” au stade retrieval avant génération.

### Fusion et recherche hybride

Vous mentionnez déjà l’usage de **RRF (Reciprocal Rank Fusion)** pour fusionner rankings lexical/sémantique. RRF est un algorithme classique de fusion de listes de résultats, décrit dans un papier SIGIR 2009, souvent apprécié car il fusionne des rangs sans devoir normaliser des scores hétérogènes. citeturn5search7turn5search3

À noter : si vous souhaitez à terme réduire la dépendance à deux moteurs séparés, Qdrant a documenté une API de requête permettant de **combiner plusieurs méthodes côté serveur** pour construire de l’hybride, et LangChain documente aussi l’intégration des sparse vectors côté Qdrant. citeturn7search14turn7search30 En parallèle, BGE-M3 annonce pouvoir fournir à la fois dense et sparse, ce qui ouvre la voie à une hybridation “unifiée” (selon vos contraintes de pertinence et vos filtres). citeturn5search0turn12search14

## Évaluation et observabilité pour améliorer le RAG sans régression

L’un des points qui différencie les piles RAG “démos” des piles “investigation/production” est la capacité à **mesurer** et **prévenir les régressions** (retrieval, citations, hallucinations, latence). L’open source est devenu très riche sur ce segment.

**Ragas** (outil + papier) se présente comme une boîte à outils pour évaluer et optimiser des applications LLM, avec génération de jeux de tests si vous n’en avez pas. citeturn3search0turn3search15 Le papier EACL (demo) formalise l’approche d’évaluation de pipelines RAG (retrieval + génération) et fournit une base scientifique/communautaire solide pour instrumenter des métriques type faithfulness/answer relevancy/context precision-context recall. citeturn3search15turn3search7

**TruLens** (MIT) propose instrumentation et évaluation pour expériences LLM et agents, avec des quickstarts centrés sur l’évaluation d’un RAG “built from scratch” et des évaluations retrieval. citeturn3search1turn3search12 Il est utile si vous voulez une boucle “score + diagnostic” plus proche de la prod (au-delà d’un notebook ponctuel).

**DeepEval** (open source) se positionne comme un framework de “unit tests” pour sorties LLM, de type Pytest mais spécialisé LLM, avec métriques incluant hallucination, answer relevancy, etc., et une doc dédiée à l’évaluation RAG. citeturn3search2turn3search9 Cela se marie très bien avec un repo structuré comme Archon (tests CI/CD sur un set de questions + assertions).

**Phoenix** par entity["company","Arize AI","ai observability company"] est une plateforme open source d’observabilité (traces OpenTelemetry), d’évaluation et d’expérimentation, explicitement pensée pour diagnostiquer prompts/retrieval/LLM. citeturn3search3turn3search14 Si vous instrumentez Archon (search → rerank → prompt → answer), Phoenix devient un “oscilloscope” pour comprendre pourquoi une réponse a dérivé après un changement de chunking ou d’embeddings.

Enfin, la “qualité RAG” doit être pensée comme un produit : si vous introduisez une UI tierce type Open WebUI, il faut intégrer au même niveau les questions de **sécurité** (advisories) et de **licence**. L’exemple Open WebUI montre à la fois une évolution de licence (branding clause) citeturn9search3 et une vulnérabilité documentée via advisory GitHub et NVD autour d’un mode de connexion externe (“Direct Connections”) citeturn11search1turn11search2 — des aspects critiques dans une plateforme d’investigation manipulant des données sensibles.

## Recommandations concrètes d’intégration dans Archon

L’objectif réaliste (et efficace) n’est généralement pas “remplacer Archon par X”, mais de **prendre les meilleurs patterns** des meilleurs projets open source, puis de les intégrer à vos abstractions existantes (scan → index → search → viewer → chat).

Votre backlog “haut impact” peut se structurer ainsi, en s’appuyant directement sur les codebases citées plus haut :

Premièrement, ajoutez un **reranker cross-encoder** après votre retrieval hybride (Meilisearch + Qdrant + RRF). Les rerankers BGE documentent précisément le schéma “retrieve top 100 puis rerank pour top 3”, et c’est un apport majeur à la précision. citeturn5search2turn5search32 Techniquement dans Archon : vous gardez votre fusion RRF, puis vous rerankez la shortlist avant construction du contexte.

Deuxièmement, améliorez la **qualité des chunks** en amont. Les outils de parsing structurants (Unstructured, Docling, Marker, MegaParse) aident à préserver structure, tables, sections et positions, ce qui sert à la fois : search, viewer, citations et graph d’entités. citeturn6search8turn6search2turn6search3turn10search36 Dans un runbook Archon, cela se traduit souvent par : “extraire en éléments structurés → normaliser → chunker par blocs logiques → stocker offsets pages/anchors → indexer”.

Troisièmement, si votre corpus est très FR/multilingue, testez des embeddings adaptés : BGE-M3 annonce 100+ langues et des capacités sparse/dense/multi-vector ; multilingual-e5-large fournit une option alternative très utilisée. citeturn5search0turn5search1 Vous pouvez conserver Gemini côté génération tout en remplaçant/complétant l’étape embeddings par un modèle self-host pour plus de contrôle (latence/coûts/souveraineté), puis mesurer l’impact avec Ragas/DeepEval.

Quatrièmement, exploitez votre “graphe d’entités” existant comme un vrai retriever : GraphRAG formalise une chaîne extraction d’entités/relations + résumés “communautaires” pour augmenter les prompts à la requête, et R2R annonce aussi des capacités knowledge graphs et “Deep Research” multi-étapes. citeturn1search9turn4search3turn4search25 Concrètement, cela ouvre des patterns “investigation” puissants : requête → expansion via entités proches → récupération de passages + résumés de communautés → réponse citée + navigation dans le graphe.

Cinquièmement, industrialisez la qualité via une **boucle d’évaluation**. Ragas (papier + lib) fournit une base métrique RAG et de génération/évaluation de datasets ; DeepEval apporte une approche “tests unitaires LLM”; Phoenix ajoute traces/diagnostic en prod. citeturn3search15turn3search2turn3search3 Pour Archon, l’implémentation la plus “copier-coller” est : (a) constituer un set de questions réalistes par projet, (b) figer des attentes minimales (citations présentes, réponses sourcées, context precision minimale), (c) exécuter en CI sur 2–3 variantes (chunking, embeddings, reranker, prompt), (d) tracer en prod les échecs.

Enfin, sur la sélection de codebases “à donner à votre IA dans l’IDE”, une règle simple est de privilégier les projets **permissifs et stables** (MIT/Apache-2.0) sauf besoin explicite d’une plateforme complète. Par exemple, Haystack (Apache-2.0) citeturn0search3turn0search0, RAGFlow (Apache-2.0) citeturn1search2turn1search6, R2R (MIT) citeturn10search3, txtai (Apache-2.0) citeturn10search1turn10search5, llmware (Apache-2.0) citeturn10search2turn10search18, Langflow (MIT) citeturn9search2, GraphRAG (MIT) citeturn8search3turn1search1. En revanche, pour Dify et Open WebUI, la documentation officielle signale des clauses/licences spécifiques ou évolutives qui méritent une revue juridique et sécurité avant intégration “core” dans une plateforme d’investigation. citeturn9search1turn9search3turn11search1